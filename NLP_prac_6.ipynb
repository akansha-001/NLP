{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP prac 6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN63rIJZK6UrC2X9tfw8Xbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akansha-001/NLP/blob/main/NLP_prac_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical No 6\n",
        "llustrate part of speech tagging.\n",
        "a. Part of speech Tagging and chunking of user defined text.\n",
        "b. Named Entity recognition of user defined text.\n",
        "c. Named Entity recognition with diagram using NLTK corpus – treebank\n",
        "POS Tagging, chunking and NER:\n",
        " \n",
        "6a) sentence tokenization, word tokenization, Part of speech Tagging and chunking of user defined text.\n",
        " \n",
        "Code"
      ],
      "metadata": {
        "id": "B4Ol4r-sRqvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "import nltk\n",
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "from nltk import chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au-W5l16Rrif",
        "outputId": "c81c0d39-fd20-48ef-c340-966219cbb60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello! this is Akansha. Today we will be learning NLP\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\",sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxg7JkA_TGlq",
        "outputId": "ced82d7d-a729-46f9-e6f9-364247a51905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Hello!', 'this is Akansha.', 'Today we will be learning NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "print(\"\\nword tokenization\\n======\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNkGuJyATNMg",
        "outputId": "05431a58-22b5-4093-aca7-34fb8a475375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word tokenization\n",
            "======\n",
            "\n",
            "['Hello', '!']\n",
            "['this', 'is', 'Akansha', '.']\n",
            "['Today', 'we', 'will', 'be', 'learning', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS TAGGING\n",
        "tagged_words=[]\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "  print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n"
      ],
      "metadata": {
        "id": "8R_AQMTOR0v2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1849c191-941e-400a-e67f-92908527fdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')], [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), ('NLP', 'NNP')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunki8ng\n",
        "tree = []\n",
        "for index in range(len(sents)):\n",
        "  tree.append(chunk.ne_chunk(tagged_words[index]))\n",
        "print(\"\\nchunking\\n===========\")\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxyc0_77TRPB",
        "outputId": "4570d643-1650-4fec-f5b5-a9093b63f2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "chunking\n",
            "===========\n",
            "[Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])]), Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])]), Tree('S', [('Today', 'NN'), ('we', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('learning', 'VBG'), Tree('ORGANIZATION', [('NLP', 'NNP')])])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6b) Named Entity recognition using user defined text.\n",
        " \n",
        "Code:\n",
        " "
      ],
      "metadata": {
        "id": "aE8ywmhPTZTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        " \n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        " \n",
        "#Process whole documnet\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "\"Google in 2007, few people outside of the company took him \"\n",
        "\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "\"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "\"this week.\")\n",
        "doc = nlp(text)\n",
        " \n",
        "#Analyze syntax\n",
        "print(\"Noun phrase: \", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verb\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        " \n",
        "print(\"Verb\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlWj3IM4TUVn",
        "outputId": "8ccc0624-50a5-4019-e007-187b273e4840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrase:  ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verb ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n",
            "Verb ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6c) Named Entity recognition with diagram using NLTK corpus – treebank.\n",
        " \n",
        "Code:\n",
        " \n",
        "Note: It runs on Python IDLE"
      ],
      "metadata": {
        "id": "QKjUOFROThfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank_chunk\n",
        "treebank_chunk.tagged_sents()[0]\n",
        "treebank_chunk.chunked_sents()[0]\n",
        "treebank_chunk.chunked_sents()[0].draw()\n"
      ],
      "metadata": {
        "id": "sP-00FAKTh4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}